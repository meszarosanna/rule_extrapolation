{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0WqCnoeU_zv"
      },
      "source": [
        "# Chance level calculations for Rule Extrapolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gSCflLhuhOHQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "import math\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h20GEZhosdmL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Code for grammar rules\n",
        "\n",
        "# these are always used\n",
        "SOS_token = np.array([0])\n",
        "EOS_token = np.array([1])\n",
        "PAD_token = np.array([2])\n",
        "\n",
        "# only for aNbNcN and variants\n",
        "A_token = np.array([3])\n",
        "B_token = np.array([4])\n",
        "C_token = np.array([5])\n",
        "\n",
        "# only for parentheses and brackets\n",
        "OPENING_PARENTHESIS_token = np.array([3])\n",
        "CLOSING_PARENTHESIS_token = np.array([4])\n",
        "OPENING_BRACKET_token = np.array([5])\n",
        "CLOSING_BRACKET_token = np.array([6])\n",
        "\n",
        "\n",
        "\n",
        "import dataclasses\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "# to_dict: creates a dictionary: {'as_before_bs_accuracy': 0.0, 'as_before_bs_completion_accuracy':0.0, etc}\n",
        "@dataclasses.dataclass\n",
        "class GrammarMetrics:\n",
        "    rule_2_accuracy: float = 0.0\n",
        "    rule_2_completion_accuracy: float = 0.0\n",
        "\n",
        "    rule_1_accuracy: float = 0.0\n",
        "    finished_accuracy: float = 0.0\n",
        "    grammatical_accuracy: float = 0.0\n",
        "\n",
        "    def to_dict(self) -> Dict[str, float]:\n",
        "        return dataclasses.asdict(self)\n",
        "\n",
        "\n",
        "# generates aNbN grammar: all sequences, all even, all odd or sequences of random length and num_samples number\n",
        "def generate_aNbN_grammar_data(\n",
        "    num_samples: int,\n",
        "    max_length: int = 32,\n",
        "    all_sequences: bool = True,\n",
        "    only_even: bool = False,\n",
        "    only_odd: bool = False,\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    PCFG with two rules:\n",
        "    - number of a's and b's must be the same\n",
        "    - a's come first, followed by b's\n",
        "\n",
        "    :param only_even: generates only sequences with even number of a's and b's\n",
        "    :param only_odd: generates only sequences with odd number of a's and b's\n",
        "    :param all_sequences: generates all sequences up to max_length (i.e., the longest will have max_length // 2 a's and b's)\n",
        "    :param num_samples: number of samples\n",
        "    :param max_length: maximum sequence length (inclusive SOS and EOS tokens)\n",
        "    :return: list of length num_samples with maximal sequences of length max_length\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if all_sequences + only_even + only_odd > 1:\n",
        "        raise ValueError(\"Only one of all_sequences, only_even, only_odd can be True\")\n",
        "\n",
        "    if all_sequences is True:\n",
        "        lengths = np.linspace(\n",
        "            start=1, stop=max_length // 2, num=max_length // 2, dtype=int, endpoint=True\n",
        "        )\n",
        "    elif only_even is True:\n",
        "        lengths = np.array(list(range(2, max_length // 2 + 1, 2)))\n",
        "    elif only_odd is True:\n",
        "        lengths = np.array(list(range(1, max_length // 2 + 1, 2)))\n",
        "    else:\n",
        "        lengths = np.random.randint(low=1, high=max_length // 2 + 1, size=num_samples)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for length in lengths:\n",
        "        data.append(\n",
        "            np.concatenate(\n",
        "                (\n",
        "                    SOS_token,\n",
        "                    A_token * np.ones(length),\n",
        "                    B_token * np.ones(length),\n",
        "                    EOS_token,\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return data  # list containing the sequences of max length max_length+2\n",
        "\n",
        "\n",
        "def generate_aNbNaN_grammar_data(\n",
        "    num_samples: int, max_length: int = 32, all_sequences=True\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    PCFG with two rules:\n",
        "    - number of a's is twice the number of b's\n",
        "    - N a's come first, followed by N b's, then N a's again\n",
        "\n",
        "    :param all_sequences:\n",
        "    :param num_samples: number of samples\n",
        "    :param max_length: maximum sequence length (inclusive SOS and EOS tokens)\n",
        "    :return: list of length num_samples with maximal sequences of length max_length\n",
        "\n",
        "    \"\"\"\n",
        "    if all_sequences is True:\n",
        "        lengths = np.linspace(\n",
        "            start=1, stop=max_length // 3, num=max_length // 3, dtype=int, endpoint=True\n",
        "        )\n",
        "    else:\n",
        "        lengths = np.random.randint(low=1, high=max_length // 3 + 1, size=num_samples)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for length in lengths:\n",
        "        data.append(\n",
        "            np.concatenate(\n",
        "                (\n",
        "                    SOS_token,\n",
        "                    A_token * np.ones(length),\n",
        "                    B_token * np.ones(length),\n",
        "                    A_token * np.ones(length),\n",
        "                    EOS_token,\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def generate_aNbNcN_grammar_data(\n",
        "    num_samples: int, max_length: int = 32, all_sequences=True\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    PCFG with two rules:\n",
        "    - number of a's is equal to the number of b's, equal to the number of c's\n",
        "    - N a's come first, followed by N b's, then N c's\n",
        "\n",
        "    :param all_sequences:\n",
        "    :param num_samples: number of samples\n",
        "    :param max_length: maximum sequence length (inclusive SOS and EOS tokens)\n",
        "    :return: list of length num_samples with maximal sequences of length max_length\n",
        "\n",
        "    \"\"\"\n",
        "    if all_sequences is True:\n",
        "        lengths = np.linspace(\n",
        "            start=1, stop=max_length // 3, num=max_length // 3, dtype=int, endpoint=True\n",
        "        )\n",
        "    else:\n",
        "        lengths = np.random.randint(low=1, high=max_length // 3 + 1, size=num_samples)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for length in lengths:\n",
        "        data.append(\n",
        "            np.concatenate(\n",
        "                (\n",
        "                    SOS_token,\n",
        "                    A_token * np.ones(length),\n",
        "                    B_token * np.ones(length),\n",
        "                    C_token * np.ones(length),\n",
        "                    EOS_token,\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def generate_abN_grammar_data(num_samples: int, max_length: int = 32) -> list:\n",
        "    \"\"\"\n",
        "    PCFG with one rule:\n",
        "    - number of a's and b's must be the same\n",
        "\n",
        "    :param num_samples: number of samples\n",
        "    :param max_length: maximum sequence length (inclusive SOS and EOS tokens)\n",
        "    :return: list of length num_samples with maximal sequences of length max_length\n",
        "    \"\"\"\n",
        "\n",
        "    lengths = np.random.randint(low=1, high=max_length // 2 + 1, size=num_samples)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for lengths in lengths:\n",
        "        abN = np.concatenate((A_token * np.ones(lengths), B_token * np.ones(lengths)))\n",
        "        # shuffle the symbols between start and end tokens\n",
        "        np.random.shuffle(abN)\n",
        "        data.append(np.concatenate((SOS_token, abN, EOS_token)))\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def generate_aNbM_grammar_data(num_samples: int, max_length: int = 32) -> list:\n",
        "    \"\"\"\n",
        "    PCFG with one rule:\n",
        "    - a's are before b's\n",
        "\n",
        "    :param num_samples: number of samples\n",
        "    :param max_length: maximum sequence length (inclusive SOS and EOS tokens)\n",
        "    :return: list of length num_samples with maximal sequences of length max_length\n",
        "    \"\"\"\n",
        "\n",
        "    lengths_a = np.random.randint(low=1, high=max_length - 2, size=num_samples)\n",
        "    lengths_b = np.ones_like(lengths_a) * max_length - lengths_a - 2\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for la, lb in zip(lengths_a, lengths_b):\n",
        "        data.append(\n",
        "            np.concatenate(\n",
        "                (SOS_token, A_token * np.ones(la), B_token * np.ones(lb), EOS_token)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def generate_bNaM_grammar_data(num_samples: int, max_length: int = 32) -> list:\n",
        "    \"\"\"\n",
        "    PCFG with one rule:\n",
        "    - b's are before a's (begins with b, without SOS, EOS)\n",
        "\n",
        "    :param num_samples: number of samples\n",
        "    :param max_length: maximum sequence length (inclusive SOS and EOS tokens)\n",
        "    :return: list of length num_samples with maximal sequences of length max_length\n",
        "    \"\"\"\n",
        "\n",
        "    lengths_b = np.random.randint(low=1, high=max_length, size=num_samples)\n",
        "    lengths_a = np.ones_like(lengths_b) * max_length - lengths_b\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for lb, la in zip(lengths_b, lengths_a):\n",
        "        data.append(np.concatenate((B_token * np.ones(la), A_token * np.ones(lb))))\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def generate_baN_grammar_data(num_samples: int, max_length: int = 32) -> list:\n",
        "    \"\"\"\n",
        "    PCFG with two rules:\n",
        "    - begins with b\n",
        "    - even number of a's\n",
        "\n",
        "    :param num_samples: number of samples\n",
        "    :param max_length: maximum sequence length (inclusive SOS and EOS tokens)\n",
        "    :return: list of length num_samples with maximal sequences of length max_length\n",
        "    \"\"\"\n",
        "\n",
        "    lengths = np.random.randint(low=1, high=max_length + 1, size=num_samples)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for l in lengths:\n",
        "        num_a = np.random.randint(low=0, high=(l - 1) // 2 + 1)\n",
        "        second_part = np.concatenate(\n",
        "            (A_token * np.ones(num_a * 2), B_token * np.ones(l - 1 - num_a * 2))\n",
        "        )\n",
        "        # shuffle the symbols\n",
        "        np.random.shuffle(second_part)\n",
        "\n",
        "        data.append(np.concatenate((SOS_token, B_token, second_part, EOS_token)))\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def generate_bbaN_grammar_data(num_samples: int, max_length: int = 32) -> list:\n",
        "    \"\"\"\n",
        "    PCFG with two rules:\n",
        "    - b's before a's ('bbbb' ok but 'aaaa' not)\n",
        "    - even number of a's\n",
        "\n",
        "    :param num_samples: number of samples\n",
        "    :param max_length: maximum sequence length (inclusive SOS and EOS tokens)\n",
        "    :return: list of length num_samples with maximal sequences of length max_length\n",
        "    \"\"\"\n",
        "\n",
        "    lengths = np.random.randint(low=1, high=max_length + 1, size=num_samples)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for l in lengths:\n",
        "        num_a = np.random.randint(low=0, high=(l - 1) // 2 + 1)\n",
        "        second_part = np.concatenate(\n",
        "            (B_token * np.ones(l - 1 - num_a * 2), A_token * np.ones(num_a * 2))\n",
        "        )\n",
        "\n",
        "        data.append(\n",
        "            np.concatenate(\n",
        "                (\n",
        "                    SOS_token,\n",
        "                    B_token * np.ones(l - num_a * 2),\n",
        "                    A_token * np.ones(num_a * 2),\n",
        "                    EOS_token,\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def pad(data: list, max_seq_length: int = 0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Pad data with PAD token\n",
        "    :param data:\n",
        "    :param max_seq_length: maximum sequence length\n",
        "    :return: numpy array with padded data of shape (batch_size, max_batch_length)\n",
        "    \"\"\"\n",
        "\n",
        "    if max_seq_length == 0:\n",
        "        # Get longest sequence in the dataset\n",
        "        for seq in data:\n",
        "            if len(seq) > max_seq_length:\n",
        "                max_seq_length = len(seq)\n",
        "\n",
        "    # Append padding tokens until it reaches the max length\n",
        "    for i, seq in enumerate(data):\n",
        "        remaining_length = max_seq_length - len(seq)\n",
        "\n",
        "        if remaining_length > 0:\n",
        "            data[i] = np.concatenate((data[i], [PAD_token.item()] * remaining_length))\n",
        "\n",
        "    return np.array(data)\n",
        "\n",
        "\n",
        "def check_as_before_bs(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the first b comes after the last a\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    if len(a_tokens := torch.where(sequence == A_token.item())[0]) > 0:\n",
        "        # find the last a\n",
        "        last_a = a_tokens[-1]\n",
        "\n",
        "        if len(b_tokens := torch.where(sequence == B_token.item())[0]) > 0:\n",
        "            # find the first b\n",
        "            first_b = b_tokens[0]\n",
        "\n",
        "            return first_b > last_a\n",
        "        else:\n",
        "            return True\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "\n",
        "def check_bs_before_as(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the first a comes after the last b. 'bbbb' ok, 'aaaa' not\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    if len(b_tokens := torch.where(sequence == B_token.item())[0]) > 0:\n",
        "        # find the last b\n",
        "        last_b = b_tokens[-1]\n",
        "\n",
        "        if len(a_tokens := torch.where(sequence == A_token.item())[0]) > 0:\n",
        "            # find the first a\n",
        "            first_a = a_tokens[0]\n",
        "\n",
        "            return first_a > last_b\n",
        "        else:\n",
        "            return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def check_as_before_cs(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the first c comes after the last a\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    if len(a_tokens := torch.where(sequence == A_token.item())[0]) > 0:\n",
        "        # find the last a\n",
        "        last_a = a_tokens[-1]\n",
        "\n",
        "        if len(c_tokens := torch.where(sequence == C_token.item())[0]) > 0:\n",
        "            # find the first c\n",
        "            first_c = c_tokens[0]\n",
        "\n",
        "            return first_c > last_a\n",
        "        else:\n",
        "            return True\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "\n",
        "def check_bs_before_cs(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the first c comes after the last b\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    if len(b_tokens := torch.where(sequence == B_token.item())[0]) > 0:\n",
        "        # find the last b\n",
        "        last_b = b_tokens[-1]\n",
        "\n",
        "        if len(c_tokens := torch.where(sequence == C_token.item())[0]) > 0:\n",
        "            # find the first c\n",
        "            first_c = c_tokens[0]\n",
        "\n",
        "            return first_c > last_b\n",
        "        else:\n",
        "            return True\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "\n",
        "def check_bs_in_the_middle(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the b's are in the middle\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    if len(b_tokens := torch.where(sequence == B_token.item())[0]) > 0:\n",
        "        # find the first b\n",
        "        first_b = b_tokens[0]\n",
        "        last_b = b_tokens[-1]\n",
        "\n",
        "        if len(sequence[:first_b]) == len(sequence[last_b + 1 :]):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def check_bs_together(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the b's are in the middle\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    if len(b_tokens := torch.where(sequence == B_token.item())[0]) > 0:\n",
        "        # find the first b\n",
        "        first_b = b_tokens[0]\n",
        "        last_b = b_tokens[-1]\n",
        "\n",
        "        if (\n",
        "            (b_subsequence := sequence[first_b : last_b + 1]) == B_token.item()\n",
        "        ).sum() == len(b_subsequence):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def check_same_number_as_bs(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the number of a's and b's is the same\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    num_as = torch.sum(sequence == A_token.item())\n",
        "    num_bs = torch.sum(sequence == B_token.item())\n",
        "    return num_as == num_bs\n",
        "\n",
        "\n",
        "def check_twice_many_as_than_bs(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the number of a's and b's is the same\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    num_as = torch.sum(sequence == A_token.item())\n",
        "    num_bs = torch.sum(sequence == B_token.item())\n",
        "    return num_as == 2 * num_bs\n",
        "\n",
        "\n",
        "def check_more_as_than_bs(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if there are more a's than b's\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    num_as = torch.sum(sequence == A_token.item())\n",
        "    num_bs = torch.sum(sequence == B_token.item())\n",
        "    return num_as >= num_bs\n",
        "\n",
        "\n",
        "def check_more_bs_than_cs(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if there are more b's than c's\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    num_bs = torch.sum(sequence == B_token.item())\n",
        "    num_cs = torch.sum(sequence == C_token.item())\n",
        "    return num_bs >= num_cs\n",
        "\n",
        "\n",
        "def check_more_as_before_bs(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if there are more a's than b's\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    if len(b_tokens := torch.where(sequence == B_token.item())[0]) > 0:\n",
        "        first_b = b_tokens[0]\n",
        "\n",
        "        num_as = torch.sum(sequence[:first_b] == A_token.item())\n",
        "        num_bs = torch.sum(sequence == B_token.item())\n",
        "        return num_as >= num_bs\n",
        "\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "\n",
        "def check_same_number_as_bs_cs(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the number of a's, b's and c's is the same\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    num_as = torch.sum(sequence == A_token.item())\n",
        "    num_bs = torch.sum(sequence == B_token.item())\n",
        "    num_cs = torch.sum(sequence == C_token.item())\n",
        "    return (num_as == num_bs) and (num_bs == num_cs)\n",
        "\n",
        "\n",
        "def check_as_before_bs_before_cs(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the first b comes after the last a and the first c comes after the last b\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    if len(c_tokens := torch.where(sequence == C_token.item())[0]) > 0:\n",
        "        # find the first c\n",
        "        first_c = c_tokens[0]\n",
        "\n",
        "        if len(b_tokens := torch.where(sequence == B_token.item())[0]) > 0:\n",
        "            # find the first and last b\n",
        "            last_b = b_tokens[-1]\n",
        "            first_b = b_tokens[0]\n",
        "\n",
        "            if len(a_tokens := torch.where(sequence == A_token.item())[0]) > 0:\n",
        "                # find the last a\n",
        "                last_a = a_tokens[-1]\n",
        "                if (last_a < first_b) and (last_b < first_c):\n",
        "                    return True\n",
        "                else:\n",
        "                    return False\n",
        "            else:\n",
        "                return check_bs_before_cs(sequence)\n",
        "        else:\n",
        "            return check_as_before_cs(sequence)\n",
        "    else:\n",
        "        return check_as_before_bs(sequence)\n",
        "\n",
        "\n",
        "def check_in_dist_anbncn(sequence: torch.Tensor):\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    if len(c_tokens := torch.where(sequence == C_token.item())[0]) == 0:\n",
        "        if len(b_tokens := torch.where(sequence == B_token.item())[0]) == 0:\n",
        "            return True\n",
        "        else:\n",
        "            return check_as_before_bs(sequence) and check_more_as_than_bs(sequence)\n",
        "    else:\n",
        "        return (\n",
        "            check_as_before_bs(sequence)\n",
        "            and check_bs_before_cs(sequence)\n",
        "            and check_same_number_as_bs(sequence)\n",
        "            and check_more_bs_than_cs(sequence)\n",
        "        )\n",
        "\n",
        "\n",
        "def check_sequence_finished(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the sequence is finished (EOS token is in the sequence)\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    # find the first EOS token\n",
        "    return len(torch.where(sequence == EOS_token.item())[0]) > 0\n",
        "\n",
        "\n",
        "def check_even_number_of_as(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the sequence has even number of a's\n",
        "    \"\"\"\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    num_as = torch.sum(sequence == A_token.item())\n",
        "\n",
        "    return num_as % 2 == 0\n",
        "\n",
        "\n",
        "def check_begins_with_b(sequence: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Check if the sequence begins with a B_token (after SOS)\n",
        "    \"\"\"\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    if sequence[0] == SOS_token.item():\n",
        "        return sequence[1] == B_token.item()\n",
        "    else:\n",
        "        return sequence[0] == B_token.item()\n",
        "\n",
        "\n",
        "def generate_test_prompts(length: int = 6, grammar: str = \"aNbN\"):\n",
        "    \"\"\"\n",
        "    Generates all prompts of a given length with symbols a and b or (and c)\n",
        "    :param length:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    num_samples = 2**length\n",
        "    if grammar in [\"aNbN\", \"abN\", \"aNbM\", \"aNbNaN\", \"baN\"]:\n",
        "        symbols = [A_token.item(), B_token.item()]\n",
        "        prompts = torch.tensor(list(product(symbols, repeat=length)), dtype=torch.long)\n",
        "\n",
        "        # add SOS\n",
        "        prompts = torch.cat(\n",
        "            (torch.ones((prompts.shape[0], 1), dtype=torch.long) * SOS_token, prompts),\n",
        "            dim=1,\n",
        "        )\n",
        "    elif grammar == \"aNbNcN\":\n",
        "        symbols = [A_token.item(), B_token.item(), C_token.item()]\n",
        "        prompts = torch.tensor(list(product(symbols, repeat=length)), dtype=torch.long)\n",
        "\n",
        "        # add SOS\n",
        "        prompts = torch.cat(\n",
        "            (torch.ones((prompts.shape[0], 1), dtype=torch.long) * SOS_token, prompts),\n",
        "            dim=1,\n",
        "        )\n",
        "    elif grammar == \"bbaN\":\n",
        "        ID_data = torch.tensor(\n",
        "            generate_bNaM_grammar_data(num_samples=num_samples // 2, max_length=length),\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "        OOD_data = torch.tensor(\n",
        "            generate_bNaM_grammar_data(\n",
        "                num_samples=num_samples // 2, max_length=length - 1\n",
        "            ),\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "        id_prompts = torch.cat(\n",
        "            (torch.ones((ID_data.shape[0], 1), dtype=torch.long) * SOS_token, ID_data),\n",
        "            dim=1,\n",
        "        )\n",
        "        ood_prompts = torch.cat(\n",
        "            (\n",
        "                torch.ones((OOD_data.shape[0], 1), dtype=torch.long) * SOS_token,\n",
        "                torch.ones((OOD_data.shape[0], 1), dtype=torch.long) * A_token,\n",
        "                OOD_data,\n",
        "            ),\n",
        "            dim=1,\n",
        "        )\n",
        "        prompts = torch.cat((ood_prompts, id_prompts), dim=0)\n",
        "\n",
        "    elif grammar == \"parentheses\":\n",
        "        data = torch.tensor(\n",
        "            generate_matched_parentheses_data(\n",
        "                num_samples=num_samples / 2, max_length=length, fixed_length=True\n",
        "            ),\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "        ood_prompts = torch.cat(\n",
        "            (\n",
        "                data[:, 0].view(-1, 1),\n",
        "                torch.ones((data.shape[0], 1), dtype=torch.long)\n",
        "                * CLOSING_PARENTHESIS_token,\n",
        "                torch.ones((data.shape[0], 1), dtype=torch.long)\n",
        "                * OPENING_PARENTHESIS_token,\n",
        "                data[:, 1:-1],\n",
        "            ),\n",
        "            dim=1,\n",
        "        )  # remove EOS\n",
        "\n",
        "        id_prompts = torch.cat(\n",
        "            (\n",
        "                data[:, 0].view(-1, 1),\n",
        "                torch.ones((data.shape[0], 1), dtype=torch.long)\n",
        "                * OPENING_PARENTHESIS_token,\n",
        "                torch.ones((data.shape[0], 1), dtype=torch.long)\n",
        "                * CLOSING_PARENTHESIS_token,\n",
        "                data[:, 1:-1],\n",
        "            ),\n",
        "            dim=1,\n",
        "        )  # remove EOS\n",
        "\n",
        "        prompts = torch.cat((ood_prompts, id_prompts), dim=0)\n",
        "    elif grammar == \"brackets\":\n",
        "        data = torch.tensor(\n",
        "            generate_matched_brackets_data(\n",
        "                num_samples=num_samples / 2, max_length=length, fixed_length=True\n",
        "            ),\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "        ood_prompts = torch.cat(\n",
        "            (\n",
        "                data[:, 0].view(-1, 1),\n",
        "                torch.ones((data.shape[0], 1), dtype=torch.long)\n",
        "                * CLOSING_BRACKET_token,\n",
        "                torch.ones((data.shape[0], 1), dtype=torch.long)\n",
        "                * OPENING_BRACKET_token,\n",
        "                data[:, 1:-1],\n",
        "            ),\n",
        "            dim=1,\n",
        "        )  # remove EOS\n",
        "\n",
        "        id_prompts = torch.cat(\n",
        "            (\n",
        "                data[:, 0].view(-1, 1),\n",
        "                torch.ones((data.shape[0], 1), dtype=torch.long)\n",
        "                * OPENING_BRACKET_token,\n",
        "                torch.ones((data.shape[0], 1), dtype=torch.long)\n",
        "                * CLOSING_BRACKET_token,\n",
        "                data[:, 1:-1],\n",
        "            ),\n",
        "            dim=1,\n",
        "        )  # remove EOS\n",
        "        prompts = torch.cat((ood_prompts, id_prompts), dim=0)\n",
        "\n",
        "    elif grammar == \"parentheses_and_brackets\":\n",
        "        data = torch.tensor(\n",
        "            generate_matched_parentheses_and_brackets_data(\n",
        "                num_samples=num_samples / 2, max_length=length, fixed_length=True\n",
        "            ),\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "        # generate torch 0-1 sequence in shape (data.shape[0], 1)\n",
        "        ood_prompts = torch.cat(\n",
        "            (\n",
        "                data[:, 0].view(-1, 1),\n",
        "                torch.ones((data.shape[0], 1), dtype=torch.long)\n",
        "                * CLOSING_PARENTHESIS_token,\n",
        "                torch.ones((data.shape[0], 1), dtype=torch.long)\n",
        "                * OPENING_PARENTHESIS_token,\n",
        "                data[:, 1:-1],\n",
        "            ),\n",
        "            dim=1,\n",
        "        )  # remove EOS\n",
        "\n",
        "        id_prompts = torch.cat(\n",
        "            (\n",
        "                data[:, 0].view(-1, 1),\n",
        "                torch.ones((data.shape[0], 1), dtype=torch.long)\n",
        "                * OPENING_PARENTHESIS_token,\n",
        "                torch.ones((data.shape[0], 1), dtype=torch.long)\n",
        "                * CLOSING_PARENTHESIS_token,\n",
        "                data[:, 1:-1],\n",
        "            ),\n",
        "            dim=1,\n",
        "        )  # remove EOS\n",
        "\n",
        "        prompts = torch.cat((ood_prompts, id_prompts), dim=0)\n",
        "    return prompts\n",
        "\n",
        "\n",
        "def grammar_rules(grammar):\n",
        "    \"\"\"\n",
        "    Selects the rules the grammar needs to satisfy.\n",
        "    :param grammar:\n",
        "    \"\"\"\n",
        "    if grammar == \"aNbN\":\n",
        "        return lambda x: check_same_number_as_bs(x) and check_as_before_bs(x)\n",
        "    elif grammar == \"aNbNcN\":\n",
        "        return lambda x: check_same_number_as_bs_cs(x) and check_as_before_bs_before_cs(\n",
        "            x\n",
        "        )\n",
        "    elif grammar == \"baN\":\n",
        "        return lambda x: check_even_number_of_as(x) and check_begins_with_b(x)\n",
        "    elif grammar == \"bbaN\":\n",
        "        return lambda x: check_even_number_of_as(x) and check_bs_before_as(x)\n",
        "    elif grammar == \"abN\":\n",
        "        return lambda x: check_same_number_as_bs(x)\n",
        "    elif grammar == \"aNbM\":\n",
        "        return lambda x: check_as_before_bs(x)\n",
        "    elif grammar == \"aNbNaN\":\n",
        "        return (\n",
        "            lambda x: check_twice_many_as_than_bs(x)\n",
        "            and check_bs_in_the_middle(x)\n",
        "            and check_bs_together(x)\n",
        "        )\n",
        "    elif grammar == \"brackets\":\n",
        "        return lambda x: check_matched_brackets(x)\n",
        "    elif grammar == \"parentheses\":\n",
        "        return lambda x: check_matched_parentheses(x)\n",
        "    elif grammar == \"parentheses_and_brackets\":\n",
        "        return lambda x: check_matched_parentheses_and_brackets(x)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown grammar {grammar}\")\n",
        "\n",
        "\n",
        "def prompt_grammar_rules(grammar):\n",
        "    \"\"\"\n",
        "    Selects the rules that check whether a prompt can be completed as such that it satisfies the rules of the grammar.\n",
        "    It is used to split the test_prompts into in-distribution and out-of-distribution.\n",
        "\n",
        "    NOTE: these rules are LESS strict than the grammar_rules, because even if the prompt does not satisfy the grammar rules,\n",
        "    it might be completed as such that it does.\n",
        "    :param grammar:\n",
        "\n",
        "    \"\"\"\n",
        "    if grammar == \"aNbN\":\n",
        "        return lambda x: check_as_before_bs(x) and check_more_as_than_bs(x)\n",
        "    elif grammar == \"aNbNcN\":\n",
        "        return lambda x: check_in_dist_anbncn(x)\n",
        "    elif grammar == \"abN\":\n",
        "        return lambda x: True\n",
        "    elif grammar == \"baN\":\n",
        "        return lambda x: check_begins_with_b(x)\n",
        "    elif grammar == \"bbaN\":\n",
        "        return lambda x: check_begins_with_b(x)\n",
        "    elif grammar == \"aNbM\":\n",
        "        return lambda x: check_as_before_bs(x)\n",
        "    elif grammar == \"aNbNaN\":\n",
        "        return lambda x: check_as_before_bs(x) and check_bs_together(x)\n",
        "    elif grammar == \"brackets\":\n",
        "        return lambda x: check_matched_brackets(x)\n",
        "    elif grammar == \"parentheses\":\n",
        "        return lambda x: check_matched_parentheses(x)\n",
        "    elif grammar == \"parentheses_and_brackets\":\n",
        "        return lambda x: check_matched_parentheses_and_brackets(x)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown grammar {grammar}\")\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "def generate_matched_parentheses_and_brackets(n):\n",
        "    \"\"\"\n",
        "    Generate a word of length n with paired () and [].\n",
        "    \"\"\"\n",
        "    if n == 0:\n",
        "        return np.concatenate((SOS_token, EOS_token))\n",
        "    elif n % 2 == 1:\n",
        "        raise ValueError(\"Length can only be even\")\n",
        "    else:\n",
        "        word = []\n",
        "        stack = []\n",
        "        while len(word) < n:  # Each pair of parentheses or brackets adds 2 characters\n",
        "            if len(stack) == 0:\n",
        "                choice = random.choice(\n",
        "                    [OPENING_PARENTHESIS_token, OPENING_BRACKET_token]\n",
        "                )\n",
        "            elif stack[-1] == OPENING_PARENTHESIS_token:\n",
        "                choice = random.choice(\n",
        "                    [\n",
        "                        OPENING_PARENTHESIS_token,\n",
        "                        OPENING_BRACKET_token,\n",
        "                        CLOSING_PARENTHESIS_token,\n",
        "                    ]\n",
        "                )\n",
        "                if len(word) + len(stack) >= n:\n",
        "                    choice = CLOSING_PARENTHESIS_token\n",
        "\n",
        "            elif stack[-1] == OPENING_BRACKET_token:\n",
        "                choice = random.choice(\n",
        "                    [\n",
        "                        OPENING_PARENTHESIS_token,\n",
        "                        OPENING_BRACKET_token,\n",
        "                        CLOSING_BRACKET_token,\n",
        "                    ]\n",
        "                )\n",
        "                if len(word) + len(stack) >= n:\n",
        "                    choice = CLOSING_BRACKET_token\n",
        "\n",
        "            if choice == OPENING_PARENTHESIS_token:\n",
        "                word.append(OPENING_PARENTHESIS_token)\n",
        "                stack.append(OPENING_PARENTHESIS_token)\n",
        "            elif choice == OPENING_BRACKET_token:\n",
        "                word.append(OPENING_BRACKET_token)\n",
        "                stack.append(OPENING_BRACKET_token)\n",
        "            elif choice == CLOSING_PARENTHESIS_token:\n",
        "                word.append(CLOSING_PARENTHESIS_token)\n",
        "                stack.pop()\n",
        "            elif choice == CLOSING_BRACKET_token:\n",
        "                word.append(CLOSING_BRACKET_token)\n",
        "                stack.pop()\n",
        "\n",
        "            if len(stack) == 0:\n",
        "                break\n",
        "\n",
        "        return np.concatenate((SOS_token, *word, EOS_token))\n",
        "\n",
        "\n",
        "def check_matched_parentheses_and_brackets(sequence: torch.Tensor) -> bool:\n",
        "    \"\"\"\n",
        "    Check if the parentheses and brackets are matched\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    stack = []\n",
        "    for token in sequence:\n",
        "        if token == OPENING_PARENTHESIS_token.item():\n",
        "            stack.append(token)\n",
        "        elif token == CLOSING_PARENTHESIS_token.item():\n",
        "            if len(stack) == 0 or stack[-1] != OPENING_PARENTHESIS_token.item():\n",
        "                return False\n",
        "            stack.pop()\n",
        "        elif token == OPENING_BRACKET_token.item():\n",
        "            stack.append(token)\n",
        "        elif token == CLOSING_BRACKET_token.item():\n",
        "            if len(stack) == 0 or stack[-1] != OPENING_BRACKET_token.item():\n",
        "                return False\n",
        "            stack.pop()\n",
        "\n",
        "    return len(stack) == 0\n",
        "\n",
        "\n",
        "def generate_matched_parentheses(n):\n",
        "    \"\"\"\n",
        "    Generate a word of length n with paired ().\n",
        "    \"\"\"\n",
        "    if n == 0:\n",
        "        return np.concatenate((SOS_token, EOS_token))\n",
        "    elif n % 2 == 1:\n",
        "        raise ValueError(\"Length can only be even\")\n",
        "    else:\n",
        "        word = []\n",
        "        stack = []\n",
        "        while len(word) < n:  # Each pair of parentheses or brackets adds 2 characters\n",
        "            if len(stack) == 0:\n",
        "                choice = OPENING_PARENTHESIS_token\n",
        "            elif stack[-1] == OPENING_PARENTHESIS_token:\n",
        "                choice = random.choice(\n",
        "                    [OPENING_PARENTHESIS_token, CLOSING_PARENTHESIS_token]\n",
        "                )\n",
        "                if len(word) + len(stack) >= n:\n",
        "                    choice = CLOSING_PARENTHESIS_token\n",
        "\n",
        "            if choice == OPENING_PARENTHESIS_token:\n",
        "                word.append(OPENING_PARENTHESIS_token)\n",
        "                stack.append(OPENING_PARENTHESIS_token)\n",
        "\n",
        "            elif choice == CLOSING_PARENTHESIS_token:\n",
        "                word.append(CLOSING_PARENTHESIS_token)\n",
        "                stack.pop()\n",
        "\n",
        "            if len(stack) == 0:\n",
        "                break\n",
        "\n",
        "        return np.concatenate((SOS_token, *word, EOS_token))\n",
        "\n",
        "\n",
        "def check_matched_parentheses(sequence: torch.Tensor) -> bool:\n",
        "    \"\"\"\n",
        "    Check if the parentheses are matched\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    stack = []\n",
        "    for token in sequence:\n",
        "        if token == OPENING_PARENTHESIS_token.item():\n",
        "            stack.append(token)\n",
        "        elif token == CLOSING_PARENTHESIS_token.item():\n",
        "            if len(stack) == 0:\n",
        "                return False\n",
        "            stack.pop()\n",
        "\n",
        "    return len(stack) == 0\n",
        "\n",
        "\n",
        "def generate_matched_brackets(n):\n",
        "    \"\"\"\n",
        "    Generate a word of length n with paired [].\n",
        "    \"\"\"\n",
        "    if n == 0:\n",
        "        return np.concatenate((SOS_token, EOS_token))\n",
        "    elif n % 2 == 1:\n",
        "        raise ValueError(\"Length can only be even\")\n",
        "    else:\n",
        "        word = []\n",
        "        stack = []\n",
        "        while len(word) < n:  # Each pair of parentheses or brackets adds 2 characters\n",
        "            if len(stack) == 0:\n",
        "                choice = OPENING_BRACKET_token\n",
        "\n",
        "            elif stack[-1] == OPENING_BRACKET_token:\n",
        "                choice = random.choice([2, CLOSING_BRACKET_token])\n",
        "                if len(word) + len(stack) >= n:\n",
        "                    choice = CLOSING_BRACKET_token\n",
        "\n",
        "            if choice == OPENING_BRACKET_token:\n",
        "                word.append(OPENING_BRACKET_token)\n",
        "                stack.append(OPENING_BRACKET_token)\n",
        "            elif choice == CLOSING_BRACKET_token:\n",
        "                word.append(CLOSING_BRACKET_token)\n",
        "                stack.pop()\n",
        "\n",
        "            if len(stack) == 0:\n",
        "                break\n",
        "\n",
        "        return np.concatenate((SOS_token, *word, EOS_token))\n",
        "\n",
        "\n",
        "def check_matched_brackets(sequence: torch.Tensor) -> bool:\n",
        "    \"\"\"\n",
        "    Check if the brackets are matched\n",
        "    :param sequence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if type(sequence) == np.ndarray:\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "    stack = []\n",
        "    for token in sequence:\n",
        "        if token == OPENING_BRACKET_token.item():\n",
        "            stack.append(token)\n",
        "        elif token == CLOSING_BRACKET_token.item():\n",
        "            if len(stack) == 0:\n",
        "                return False\n",
        "            stack.pop()\n",
        "\n",
        "    return len(stack) == 0\n",
        "\n",
        "\n",
        "def generate_matched_parentheses_data(\n",
        "    num_samples: int, max_length: int = 32, fixed_length=False\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    :param num_samples: number of samples\n",
        "    :param max_length: maximum sequence length (inclusive SOS and EOS tokens)\n",
        "    :return: list of length num_samples with maximal sequences of length max_length\n",
        "    \"\"\"\n",
        "\n",
        "    if fixed_length is False:\n",
        "        lengths = np.random.randint(low=1, high=max_length // 2 + 1, size=num_samples)\n",
        "        data = [generate_matched_parentheses(2 * l) for l in lengths]\n",
        "    else:\n",
        "        data = []\n",
        "        while len(data) < num_samples:\n",
        "            sample = generate_matched_parentheses(max_length)\n",
        "            if len(sample) == (max_length + 2):  # +SOS, EOS\n",
        "                data.append(sample)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def generate_matched_brackets_data(\n",
        "    num_samples: int, max_length: int = 32, fixed_length=False\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    :param num_samples: number of samples\n",
        "    :param max_length: maximum sequence length (inclusive SOS and EOS tokens)\n",
        "    :return: list of length num_samples with maximal sequences of length max_length\n",
        "    \"\"\"\n",
        "\n",
        "    if fixed_length is False:\n",
        "        lengths = np.random.randint(low=1, high=max_length // 2 + 1, size=num_samples)\n",
        "        data = [generate_matched_brackets(2 * l) for l in lengths]\n",
        "    else:\n",
        "        data = []\n",
        "        while len(data) < num_samples:\n",
        "            sample = generate_matched_parentheses(max_length)\n",
        "            if len(sample) == (max_length + 2):  # +SOS, EOS\n",
        "                data.append(sample)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def generate_matched_parentheses_and_brackets_data(\n",
        "    num_samples: int, max_length: int = 32, fixed_length=False\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    :param num_samples: number of samples\n",
        "    :param max_length: maximum sequence length (inclusive SOS and EOS tokens)\n",
        "    :return: list of length num_samples with maximal sequences of length max_length\n",
        "    \"\"\"\n",
        "\n",
        "    if fixed_length is False:\n",
        "        lengths = np.random.randint(low=1, high=max_length // 2 + 1, size=num_samples)\n",
        "        data = [generate_matched_parentheses_and_brackets(2 * l) for l in lengths]\n",
        "    else:\n",
        "        data = []\n",
        "        while len(data) < num_samples:\n",
        "            sample = generate_matched_parentheses_and_brackets(max_length)\n",
        "            if len(sample) == (max_length + 2):  # +SOS, EOS\n",
        "                data.append(sample)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dyck language"
      ],
      "metadata": {
        "id": "RMImFX0gaeKP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4_wQ4Lmi0lR",
        "outputId": "00389d14-d458-4a34-f7ba-ddc572e21d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1/1 [01:11<00:00, 71.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num to close: 1 0.12732200375000433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1/1 [00:53<00:00, 53.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num to close: 0 0.38196601125004565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def brackets(num_to_close=1):\n",
        "  tot=0\n",
        "  for l in tqdm(range(299, 300)):\n",
        "    for n in range(num_to_close, l+1):\n",
        "      for i in range(0, (n-num_to_close)//2+1):\n",
        "        for k in range(0, n-2*i-num_to_close+1):\n",
        "          tot+=math.comb(n, 2*i + num_to_close) * math.comb(n-2*i-num_to_close, k) * math.comb(2*i, i)*(1/(i+1))*(1/(5**(n+1)))\n",
        "  return tot\n",
        "print(f\"Num to close: 1 {brackets(1)}\")\n",
        "print(f\"Num to close: 0 {brackets(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##aNbNcN"
      ],
      "metadata": {
        "id": "ksgbZWD-Y5vn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltMlOIXOhkwB",
        "outputId": "7f4e60b1-cedd-48fc-8d39-e2cc487b5245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 3, 3, 3, 3, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1/1 [00:05<00:00,  5.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 3, 3, 3, 3, 4])\n",
            "tensor([0, 3, 3, 3, 4, 4])\n",
            "tensor([0, 3, 3, 4, 4, 5])\n",
            "4\n",
            "0.02165618851160017\n",
            "0.03342730665586524\n",
            "0.4537037037036963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5925925925925722\n",
            "0.02165618851160017 0.03342730665586524 0.4537037037036963 0.5925925925925722\n"
          ]
        }
      ],
      "source": [
        "def calc_distances(prompt):\n",
        "  num_3 = sum((prompt==3)).item()\n",
        "  num_4 = sum((prompt==4)).item()\n",
        "  num_5 = sum((prompt==5)).item()\n",
        "  max_num = max(num_3, num_4, num_5)\n",
        "  distances = list(sorted([max_num-num_3, max_num-num_4, max_num-num_5]))\n",
        "  distances = distances[1:]\n",
        "  return tuple(distances)\n",
        "\n",
        "\n",
        "def r2_formula(prompt):\n",
        "  n = 307-len(prompt[1:])\n",
        "  acc = 0\n",
        "  if prompt[-1]==5:\n",
        "    for i in range(0, n+1):\n",
        "      acc += 1/(4**(i+1))\n",
        "  elif prompt[-1]==4:\n",
        "    for i in range(0, n+1):\n",
        "      for b in range(0, i+1):\n",
        "        acc += 1/(4**(i+1))\n",
        "  elif prompt[-1]==3:\n",
        "    for i in range(0, n+1):\n",
        "      for a in range(0, i+1):\n",
        "        for b in range(0, i-a+1):\n",
        "          acc += 1/(4**(i+1))\n",
        "  else:\n",
        "    raise ValueError\n",
        "  return acc\n",
        "\n",
        "def r2_ood(len_n):\n",
        "  tot = 0\n",
        "  all = 0\n",
        "  for lists in len_n:\n",
        "    n, number = lists[0], lists[1]\n",
        "    all += number\n",
        "    for i in range(0, n+1):\n",
        "      for a in range(0, i+1):\n",
        "        for b in range(0, i-a+1):\n",
        "          tot += 1/(4**(i+1)) * number\n",
        "  return tot/all\n",
        "\n",
        "\n",
        "def r1_formula(N1, N2, n):\n",
        "  tot = 0\n",
        "  for m in range(0, (n-(N1+N2))//3 + 1):\n",
        "    tot += math.comb(N1 + N2 + 3 * m, N1 + m)* math.comb(N2 + 2 * m, m)*(1/(4**(N1 + N2 + 3 * m + 1)))\n",
        "  return tot\n",
        "\n",
        "def aNbNcN():\n",
        "  my_dict_id={}\n",
        "  my_dict_ood={}\n",
        "  id_prompts = []\n",
        "  ood_prompts = []\n",
        "  id_r1_acc = 0\n",
        "  ood_r1_acc = 0\n",
        "  id_r2_accuracy = []\n",
        "  len_n = []\n",
        "  for i in tqdm(range(5, 6)):\n",
        "    test_prompts = generate_test_prompts(i, \"aNbNcN\")\n",
        "    for prompt in test_prompts:\n",
        "      if check_in_dist_anbncn(prompt):\n",
        "        id_prompts.append(prompt)\n",
        "      else:\n",
        "        ood_prompts.append(prompt)\n",
        "\n",
        "    for prompt in id_prompts:\n",
        "      print(prompt)\n",
        "      r2 = r2_formula(prompt)\n",
        "      id_r2_accuracy.append(r2)\n",
        "      n = 307-len(prompt[1:])\n",
        "      distances = calc_distances(prompt)\n",
        "      #print(f\"id {prompt}, dist {distances}\")\n",
        "      distances += (n,)\n",
        "\n",
        "\n",
        "      if distances not in my_dict_id.keys():\n",
        "        my_dict_id[distances]=1\n",
        "      else:\n",
        "        my_dict_id[distances]+=1\n",
        "\n",
        "    for prompt in ood_prompts:\n",
        "        n = 307-len(prompt[1:])\n",
        "        if [n, len(ood_prompts)] not in len_n:\n",
        "          len_n.append([n, len(ood_prompts)])\n",
        "        distances = calc_distances(prompt)\n",
        "        #print(f\" ood {prompt}, dist {distances}\")\n",
        "        distances += (n,)\n",
        "        if distances not in my_dict_ood.keys():\n",
        "          my_dict_ood[distances]=1\n",
        "        else:\n",
        "          my_dict_ood[distances]+=1\n",
        "\n",
        "  for distances in my_dict_id.keys():\n",
        "        id_r1_acc += r1_formula(distances[0], distances[1], distances[2]) * my_dict_id[distances]\n",
        "        #print(f\"Adding id len={(distances[2]-distances[1]-distances[0])//3}, number in set {my_dict_id[distances]} of distances {distances}, and prob {r1_formula(distances[0], distances[1], distances[2])}\")\n",
        "\n",
        "  id_r1_acc /= sum([my_dict_id[distances] for distances in my_dict_id.keys()])\n",
        "  print(sum([my_dict_id[distances] for distances in my_dict_id.keys()]))\n",
        "  print(id_r1_acc)\n",
        "\n",
        "  for distances in my_dict_ood.keys():\n",
        "        ood_r1_acc += r1_formula(distances[0], distances[1], distances[2]) * my_dict_ood[distances]\n",
        "        #print(f\"Adding len={(distances[2]-distances[1]-distances[0])//3}, number in set {my_dict_ood[distances]} of distances {distances}, and prob {r1_formula(distances[0], distances[1], distances[2])}\")\n",
        "  ood_r1_acc /= sum([my_dict_ood[distances] for distances in my_dict_ood.keys()])\n",
        "  print(ood_r1_acc)\n",
        "\n",
        "  #print(id_r2_accuracy)\n",
        "  id_r2_accuracy = sum(id_r2_accuracy)/len(id_r2_accuracy)\n",
        "  print(id_r2_accuracy)\n",
        "\n",
        "  ood_r2_acc = r2_ood(len_n)\n",
        "  print(ood_r2_acc)\n",
        "\n",
        "  return id_r1_acc, ood_r1_acc, id_r2_accuracy, ood_r2_acc\n",
        "\n",
        "id_r1_acc, ood_r1_acc, id_r2_acc, ood_r2_acc = aNbNcN()\n",
        "print(id_r1_acc, ood_r1_acc, id_r2_acc, ood_r2_acc)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##aNbN"
      ],
      "metadata": {
        "id": "4iK7zbm0ZARL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_cIvx7BWuw6",
        "outputId": "ddcd71e3-4460-47c9-dc02-a30dcb4adab4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1/1 [00:00<00:00,  5.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.10471443673912728\n",
            "0.1539634577351729\n",
            "5\n",
            "0.3555555555555554\n",
            "0.44444444444444486\n",
            "0.10471443673912728 0.1539634577351729 0.3555555555555554 0.44444444444444486\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def calc_distances(prompt):\n",
        "  num_3 = sum((prompt==3)).item()\n",
        "  num_4 = sum((prompt==4)).item()\n",
        "  N = max(num_3, num_4)-min(num_3, num_4)\n",
        "  return N\n",
        "\n",
        "\n",
        "def r2_formula(prompt):\n",
        "  n = 307-len(prompt[1:])\n",
        "  acc = 0\n",
        "  if prompt[-1]==4:\n",
        "    for i in range(0, n+1):\n",
        "      acc += 1/(4**(i+1))\n",
        "  elif prompt[-1]==3:\n",
        "    for i in range(0, n+1):\n",
        "      for a in range(0, i+1):\n",
        "        acc += 1/(4**(i+1))\n",
        "  else:\n",
        "    raise ValueError\n",
        "  return acc\n",
        "\n",
        "def r2_ood(len_n):\n",
        "  tot = 0\n",
        "  all = 0\n",
        "  for lists in len_n:\n",
        "    n, number = lists[0], lists[1]\n",
        "    all += number\n",
        "    for i in range(0, n+1):\n",
        "      for a in range(0, i+1):\n",
        "          tot += 1/(4**(i+1)) * number\n",
        "  return tot/all\n",
        "\n",
        "\n",
        "def r1_formula(N, n):\n",
        "  tot = 0\n",
        "  for m in range(0, (n-(N))//2 + 1):\n",
        "    tot += math.comb(N + 2 * m, m) *(1/(3**(N + 2 * m + 1)))\n",
        "  return tot\n",
        "\n",
        "def aNbN():\n",
        "  my_dict_id={}\n",
        "  my_dict_ood={}\n",
        "  id_prompts = []\n",
        "  ood_prompts = []\n",
        "  id_r1_acc = 0\n",
        "  ood_r1_acc = 0\n",
        "  id_r2_accuracy = []\n",
        "  len_n = []\n",
        "  for i in tqdm(range(8, 9)):\n",
        "    test_prompts = generate_test_prompts(i, \"aNbN\")\n",
        "    for prompt in test_prompts:\n",
        "      if prompt_grammar_rules(\"aNbN\")(prompt):\n",
        "\n",
        "        id_prompts.append(prompt)\n",
        "      else:\n",
        "        ood_prompts.append(prompt)\n",
        "\n",
        "    for prompt in id_prompts:\n",
        "      r2 = r2_formula(prompt)\n",
        "      id_r2_accuracy.append(r2)\n",
        "      n = 307-len(prompt[1:])\n",
        "      N = calc_distances(prompt)\n",
        "      distances = tuple((N, n))\n",
        "\n",
        "      if distances not in my_dict_id.keys():\n",
        "        my_dict_id[distances]=1\n",
        "      else:\n",
        "        my_dict_id[distances]+=1\n",
        "\n",
        "    for prompt in ood_prompts:\n",
        "        n = 307-len(prompt[1:])\n",
        "        if [n, len(ood_prompts)] not in len_n:\n",
        "          len_n.append([n, len(ood_prompts)])\n",
        "        N = calc_distances(prompt)\n",
        "        distances = tuple((N, n))\n",
        "        if distances not in my_dict_ood.keys():\n",
        "          my_dict_ood[distances]=1\n",
        "        else:\n",
        "          my_dict_ood[distances]+=1\n",
        "\n",
        "  for distances in my_dict_id.keys():\n",
        "        id_r1_acc += r1_formula(distances[0], distances[1]) * my_dict_id[distances]\n",
        "  id_r1_acc /= sum([my_dict_id[distances] for distances in my_dict_id.keys()])\n",
        "  print(id_r1_acc)\n",
        "\n",
        "  for distances in my_dict_ood.keys():\n",
        "        ood_r1_acc += r1_formula(distances[0], distances[1]) * my_dict_ood[distances]\n",
        "        #print(f\"Adding N={distances[0]}, len={(distances[1]-distances[0])//2}, acc={r1_formula(distances[0], distances[1])} this many times: {my_dict_ood[distances]}\")\n",
        "  ood_r1_acc /= sum([my_dict_ood[distances] for distances in my_dict_ood.keys()])\n",
        "  print(ood_r1_acc)\n",
        "  print(len(id_r2_accuracy))\n",
        "  id_r2_accuracy = sum(id_r2_accuracy)/len(id_r2_accuracy)\n",
        "\n",
        "  print(id_r2_accuracy)\n",
        "\n",
        "  ood_r2_acc = r2_ood(len_n)\n",
        "  print(ood_r2_acc)\n",
        "\n",
        "\n",
        "\n",
        "  return id_r1_acc, ood_r1_acc, id_r2_accuracy, ood_r2_acc, my_dict_ood, my_dict_id\n",
        "\n",
        "id_r1_acc, ood_r1_acc, id_r2_acc, ood_r2_acc, my_dict_ood, my_dict_id = aNbN()\n",
        "print(id_r1_acc, ood_r1_acc, id_r2_acc, ood_r2_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##bbaN"
      ],
      "metadata": {
        "id": "hw4F96YLZhVD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sUHiF7tbdAM",
        "outputId": "60e815a9-62dd-494b-dc78-187eaed29024"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1/1 [00:32<00:00, 32.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.24999999999993502\n",
            "0.4733333333333011\n",
            "0.24999999999996164\n",
            "0.4999999999999653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def generate_bbaN(length: int = 6):\n",
        "\n",
        "        ID_data = torch.tensor(\n",
        "            generate_bNaM_grammar_data(100 // 2, max_length=length),\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "\n",
        "        id_prompts = torch.cat(\n",
        "            (torch.ones((ID_data.shape[0], 1), dtype=torch.long) * SOS_token, ID_data),\n",
        "            dim=1,\n",
        "        )\n",
        "        return id_prompts\n",
        "def generate_bbaN_ood(length: int=8):\n",
        "        OOD_data = torch.tensor(\n",
        "            generate_bNaM_grammar_data(\n",
        "                num_samples=100 // 2, max_length=length - 1\n",
        "            ),\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "\n",
        "        ood_prompts = torch.cat(\n",
        "            (\n",
        "                torch.ones((OOD_data.shape[0], 1), dtype=torch.long) * SOS_token,\n",
        "                torch.ones((OOD_data.shape[0], 1), dtype=torch.long) * A_token,\n",
        "                OOD_data,\n",
        "            ),\n",
        "            dim=1,\n",
        "        )\n",
        "        return ood_prompts\n",
        "\n",
        "\n",
        "\n",
        "def bbaN():\n",
        "  id_r1_acc = 0\n",
        "  ood_r1_acc\n",
        "\n",
        "  all_prompt = 0\n",
        "  all_ood_prompt=0\n",
        "\n",
        "  for i in tqdm(range(8, 9)):\n",
        "    id_prompts = generate_bbaN(i)\n",
        "    ood_prompts = generate_bbaN_ood(i)\n",
        "\n",
        "    for prompt in id_prompts:\n",
        "      all_prompt +=1\n",
        "      n = 307-len(prompt[1:])\n",
        "      if 3 in prompt:\n",
        "        for a in range(0, n+1):\n",
        "          id_r1_acc += 1/(4*(n+1))\n",
        "      else:\n",
        "        for i in range(0, n+1):\n",
        "          for b in range(0, n+1):\n",
        "            id_r1_acc += 1/(4**(n+1))\n",
        "\n",
        "    for prompt in ood_prompts:\n",
        "      all_ood_prompt +=1\n",
        "      n = 307-len(prompt[1:])\n",
        "      if 3 in prompt:\n",
        "        for a in range(0, n+1):\n",
        "          ood_r1_acc += 1/(4*(n+1))\n",
        "      else:\n",
        "        for i in range(0, n+1):\n",
        "          for b in range(0, n+1):\n",
        "            ood_r1_acc += 1/(4**(n+1))\n",
        "\n",
        "  return id_r1_acc / all_prompt, ood_r1_acc/all_prompt\n",
        "\n",
        "def bbaN_even_a():\n",
        "  id_prompts = []\n",
        "  ood_prompts = []\n",
        "  id_r1_acc = 0\n",
        "  id_r2_acc = 0\n",
        "  ood_r1_acc =  0\n",
        "  ood_r2_acc = 0\n",
        "\n",
        "  all_prompt = 0\n",
        "  all_ood_prompt = 0\n",
        "\n",
        "  for i in tqdm(range(8, 9)):\n",
        "    id_prompts = generate_bbaN(i)\n",
        "    ood_prompts = generate_bbaN_ood(i)\n",
        "\n",
        "    for prompt in id_prompts:\n",
        "      all_prompt +=1\n",
        "      num_a = sum((prompt==3))\n",
        "\n",
        "      n = 307-len(prompt[1:])\n",
        "      if num_a % 2 ==0:\n",
        "        for l in range(0, n+1):\n",
        "          for a in range(0, l//2+1):\n",
        "            id_r2_acc += math.comb(l, 2*a)*(1/3**(l+1))\n",
        "      else:\n",
        "        for l in range(1, n+1):\n",
        "          for a in range(1, (l+1)//2 + 1):\n",
        "            id_r2_acc += math.comb(l, 2*a-1)*(1/3**(l+1))\n",
        "\n",
        "      if 3 in prompt:\n",
        "        for a in range(0, n+1):\n",
        "          id_r1_acc += 1/(4*(n+1))\n",
        "      else:\n",
        "        for l in range(0, n+1):\n",
        "          for b in range(0, l+1):\n",
        "            id_r1_acc += 1/(4**(l+1))\n",
        "\n",
        "    for prompt in ood_prompts:\n",
        "      all_ood_prompt +=1\n",
        "      num_a = sum((prompt==3))\n",
        "\n",
        "      n = 307-len(prompt[1:])\n",
        "      if num_a % 2 ==0:\n",
        "        for l in range(0, n+1):\n",
        "          for a in range(0, l//2+1):\n",
        "            ood_r2_acc += math.comb(l, 2*a)*(1/3**(l+1))\n",
        "      else:\n",
        "        for l in range(1, n+1):\n",
        "          for a in range(1, (l+1)//2 + 1):\n",
        "            ood_r2_acc += math.comb(l, 2*a-1)*(1/3**(l+1))\n",
        "\n",
        "      for l in range(1, n+1):\n",
        "        for b in range(1, l+1):\n",
        "          ood_r1_acc += 1/(3**(l+1))\n",
        "\n",
        "\n",
        "  return id_r1_acc / all_prompt, id_r2_acc / all_prompt, ood_r1_acc / all_ood_prompt, ood_r2_acc / all_ood_prompt\n",
        "\n",
        "r1, r2, ood_r1, ood_r2 = bbaN_even_a()\n",
        "print(r1)\n",
        "print(r2)\n",
        "print(ood_r1)\n",
        "print(ood_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##baN"
      ],
      "metadata": {
        "id": "J9wXhaddZqsN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzA2t0YGV8bm",
        "outputId": "6e6e8165-5f46-4edb-fd3d-18b2585628fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4999999999998808\n",
            "0.4999999999998808\n"
          ]
        }
      ],
      "source": [
        "def baN():\n",
        "  id = []\n",
        "  ood = []\n",
        "  all_ood_prompt = 0\n",
        "  ood_r2_acc = 0\n",
        "  all_id_prompt = 0\n",
        "  id_r2_acc = 0\n",
        "\n",
        "  test_prompts = generate_test_prompts(length=8, grammar=\"baN\")\n",
        "  for prompt in test_prompts:\n",
        "    if prompt_grammar_rules(\"baN\")(prompt):\n",
        "      id.append(prompt)\n",
        "    else:\n",
        "      ood.append(prompt)\n",
        "  for prompt in ood:\n",
        "      all_ood_prompt +=1\n",
        "      num_a = sum((prompt==3))\n",
        "\n",
        "      n = 307-len(prompt[1:])\n",
        "      if num_a % 2 ==0:\n",
        "        for l in range(0, n+1):\n",
        "          for a in range(0, l//2+1):\n",
        "            ood_r2_acc += math.comb(l, 2*a)*(1/3**(l+1))\n",
        "      else:\n",
        "        for l in range(1, n+1):\n",
        "          for a in range(1, (l+1)//2 + 1):\n",
        "            ood_r2_acc += math.comb(l, 2*a-1)*(1/3**(l+1))\n",
        "  for prompt in id:\n",
        "      all_id_prompt +=1\n",
        "      num_a = sum((prompt==3))\n",
        "\n",
        "      n = 307-len(prompt[1:])\n",
        "      if num_a % 2 ==0:\n",
        "        for l in range(0, n+1):\n",
        "          for a in range(0, l//2+1):\n",
        "            id_r2_acc += math.comb(l, 2*a)*(1/3**(l+1))\n",
        "      else:\n",
        "        for l in range(1, n+1):\n",
        "          for a in range(1, (l+1)//2 + 1):\n",
        "            id_r2_acc += math.comb(l, 2*a-1)*(1/3**(l+1))\n",
        "  return id_r2_acc / all_id_prompt, ood_r2_acc / all_ood_prompt\n",
        "\n",
        "id, ood = baN()\n",
        "print(id)\n",
        "print(ood)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}